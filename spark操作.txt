歷史記錄服務器
http://192.168.1.135:18088/
單機狀態
http://192.168.1.135:8080/
job狀態
http://192.168.1.135:4040/

cloudrea spark路徑
啟動spark
1.建立資料夾
/var/run/spark/work
2.到下面路徑
/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/sbin
3.啟動指令(由namenode啟動)
./start-all.sh

/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/bin

/pyspark --master spark://nn01:7077
測試
from pyspark import SparkContext
sc = SparkContext()
lines = sc.textFile("hdfs://192.168.1.135/0422_sqprk_test")
words = lines.flatMap(lambda x: x.split(" "))
word_counts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y) word_counts.saveAsTextFile("hdfs://192.168.1.135/0422_sqprk_test/out")



========================================================================================
from pyspark import SparkContext

if __name__ == "__main__":
    sc = SparkContext()
#開啟hdfs檔案
#ex 
#This is a
#book This book
#is good
#["This is a","book This book","is good"]
    lines = sc.textFile("hdfs://localhost/user/cloudera/spark101/wordcount/book")
    
#flatMap攤平拆字
#["This", "is" , "a","book" , "This" , "book","is" , "good"]
    words = lines.flatMap(lambda x: x.split(" "))
    
#用map把rdd 轉為 (字,1)
#[(This,1), (is,1) , (a,1),(book,1) , (This,1) , (book,1),(is,1) , (good,1)]
    word_counts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)
    
#reduce by key
#[(This,2), (is,2) , (a,1),(book,2) , (good,1)]
    word_counts.saveAsTextFile("hdfs://localhost/user/cloudera/spark101/wordcount/output")
